{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanvi2419/INF05731_assignment1/blob/main/INFO5731_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Wednesday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (40 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.\n",
        "\n",
        "(4) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jDyTKYs-yGit"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "class DenshoScraper:\n",
        "    def __init__(self, base_url):\n",
        "        self.base_url = base_url\n",
        "        self.headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "        self.data_list = []\n",
        "\n",
        "    def scrape_page(self, page_number):\n",
        "        url = f'{self.base_url}{page_number}/'\n",
        "        req = urllib.request.Request(url, headers=self.headers)\n",
        "\n",
        "        try:\n",
        "            with urllib.request.urlopen(req) as response:\n",
        "                page_content = response.read()\n",
        "\n",
        "            soup = BeautifulSoup(page_content, 'html.parser')\n",
        "            name = soup.find('h1').text.strip()\n",
        "            description = soup.find('p').text.strip()\n",
        "            self.data_list.append({'Name': name, 'Description': description})\n",
        "            time.sleep(0.001)\n",
        "\n",
        "        except urllib.error.HTTPError as e:\n",
        "            pass\n",
        "\n",
        "    def scrape_range(self, start_page, end_page):\n",
        "        for page_number in range(start_page, end_page + 1):\n",
        "            self.scrape_page(page_number)\n",
        "\n",
        "    def save_to_csv(self, filename):\n",
        "        df = pd.DataFrame(self.data_list)\n",
        "        df.to_csv(filename, index=False)\n",
        "\n",
        "# Usage:\n",
        "base_url = 'https://ddr.densho.org/narrators/'\n",
        "scraper = DenshoScraper(base_url)\n",
        "scraper.scrape_range(1, 905)\n",
        "scraper.save_to_csv('scrapped.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (30 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('scrapped.csv')\n",
        "\n",
        "# Define functions for text cleaning steps\n",
        "def remove_noise(text):\n",
        "    try:\n",
        "        return ''.join([char for char in text if char not in string.punctuation])\n",
        "    except TypeError:\n",
        "        return ''\n",
        "\n",
        "def remove_numbers(text):\n",
        "    try:\n",
        "        return ''.join([char for char in text if not char.isdigit()])\n",
        "    except TypeError:\n",
        "        return ''\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    try:\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        return ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
        "    except AttributeError:\n",
        "        return ''\n",
        "\n",
        "def text_lowercase(text):\n",
        "    try:\n",
        "        return text.lower()\n",
        "    except AttributeError:\n",
        "        return ''\n",
        "\n",
        "def stemming(text):\n",
        "    try:\n",
        "        ps = PorterStemmer()\n",
        "        return ' '.join([ps.stem(word) for word in text.split()])\n",
        "    except AttributeError:\n",
        "        return ''\n",
        "\n",
        "def lemmatization(text):\n",
        "    try:\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "    except AttributeError:\n",
        "        return ''\n",
        "\n",
        "# Clean the 'Description' column\n",
        "df['Cleaned_Description'] = df['Description'].apply(remove_noise)\n",
        "df['Cleaned_Description'] = df['Cleaned_Description'].apply(remove_numbers)\n",
        "df['Cleaned_Description'] = df['Cleaned_Description'].apply(remove_stopwords)\n",
        "df['Cleaned_Description'] = df['Cleaned_Description'].apply(text_lowercase)\n",
        "df['Cleaned_Description'] = df['Cleaned_Description'].apply(stemming)\n",
        "df['Cleaned_Description'] = df['Cleaned_Description'].apply(lemmatization)\n",
        "\n",
        "# Save the DataFrame to a CSV file with cleaned data\n",
        "df.to_csv('scrapped_cleaned.csv', index=False)"
      ],
      "metadata": {
        "id": "8o9TwE4xRe9R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3a8619a-9391-4f40-cb00-10c471463f04"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "bh5usP4JYjmM",
        "outputId": "f1c66cd7-4b4f-4197-b14a-18e420dbcce6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               Name                                        Description  \\\n",
              "0       Gene Akutsu  Nisei male. Born September 23, 1925, in Seattl...   \n",
              "1        Jim Akutsu  Nisei male. Born January 25, 1920, in Seattle,...   \n",
              "2     Terry Aratani  Nisei male. During World War II, served with I...   \n",
              "3     Kenneth Okuma  Nisei male. Born September 19, 1917, in Hanape...   \n",
              "4  Yone Bartholomew  Nisei female. Born April 12, 1904, in Bedderav...   \n",
              "\n",
              "                                 Cleaned_Description  \n",
              "0  nisei male born septemb seattl washington spen...  \n",
              "1  nisei male born januari seattl washington inca...  \n",
              "2  nisei male world war ii serv compani part nd r...  \n",
              "3  nisei male born septemb hanapep hawaii world w...  \n",
              "4  nisei femal born april bedderavia california g...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-88354010-0c31-4aaa-b12c-945551eb0636\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Description</th>\n",
              "      <th>Cleaned_Description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Gene Akutsu</td>\n",
              "      <td>Nisei male. Born September 23, 1925, in Seattl...</td>\n",
              "      <td>nisei male born septemb seattl washington spen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Jim Akutsu</td>\n",
              "      <td>Nisei male. Born January 25, 1920, in Seattle,...</td>\n",
              "      <td>nisei male born januari seattl washington inca...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Terry Aratani</td>\n",
              "      <td>Nisei male. During World War II, served with I...</td>\n",
              "      <td>nisei male world war ii serv compani part nd r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Kenneth Okuma</td>\n",
              "      <td>Nisei male. Born September 19, 1917, in Hanape...</td>\n",
              "      <td>nisei male born septemb hanapep hawaii world w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Yone Bartholomew</td>\n",
              "      <td>Nisei female. Born April 12, 1904, in Bedderav...</td>\n",
              "      <td>nisei femal born april bedderavia california g...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-88354010-0c31-4aaa-b12c-945551eb0636')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-88354010-0c31-4aaa-b12c-945551eb0636 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-88354010-0c31-4aaa-b12c-945551eb0636');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-388bcf12-e523-4762-b8d1-5ba0fba98edc\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-388bcf12-e523-4762-b8d1-5ba0fba98edc')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-388bcf12-e523-4762-b8d1-5ba0fba98edc button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 836,\n  \"fields\": [\n    {\n      \"column\": \"Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 835,\n        \"samples\": [\n          \"Larry R. Pacheco\",\n          \"Tsuchino Forrester\",\n          \"Yukiko Katayama Omoto\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 831,\n        \"samples\": [\n          \"Nisei female. Born June 17, 1927, in Mayfield, California. Grew up in the Mountain View area, where family ran a berry farm. After Japan bombed Pearl Harbor, family decided to move to Utah to avoid mass removal. Attended high school in Utah before returning to California to enroll in San Jose State University. Remained in San Jose.\",\n          \"Shin-Issei female. Born March 30, 1931 in Kasuga, Fukuoka, Japan. Grew up in Kasuga where parents ran a farm. Graduated from a girl's high school during the U.S. occupation of Japan. Met future husband, a U.S. serviceman, and immigrated to the United States as a \\\"war bride.\\\"\",\n          \"Nisei male. Born November 12, 1931, in San Jose, California. During World War II, removed to Heart Mountain concentration camp, Wyoming. Graduated from the University of California at Berkeley, and served in the military as an intelligence officer in Japan and Korea. Served on the San Jose City Council from 1967 to 1971, and as mayor of San Jose from 1967 to 1971. Served as U.S. Congressman from 1975 to 1995. While in Congress, was integral in the passage of H.R. 442, the Civil Liberties Act of 1988. Served as Secretary of Transportation from 2001 to 2006.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Cleaned_Description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 829,\n        \"samples\": [\n          \"nisei male born june lima peru parent origin immigr peru okinawa japan world war ii remov famili crystal citi intern camp texa leav crystal citi move kauai hawaii establish success career architect\",\n          \"nisei male born april san diego california grew termin island california father fisherman world war ii remov famili santa anita assembl center california heart mountain concentr camp wyom famili transfer tule lake respons father answer socal loyalti questionnair turn eighteen tule lake also sign nono questionnair renounc u citizenship sent fort lincoln bismarck intern camp north dakota go japan reunit famili live japan number year take job u armi volunt u air forc eventu regain citizenship return u\",\n          \"nisei male born novemb san jose california world war ii remov heart mountain concentr camp wyom graduat univers california berkeley serv militari intellig offic japan korea serv san jose citi council mayor san jose serv u congressman congress integr passag hr civil liberti act serv secretari transport\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (30 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load the cleaned text from the CSV file\n",
        "df = pd.read_csv('scrapped_cleaned.csv')\n",
        "clean_texts = df['Cleaned_Description'].tolist()\n",
        "\n",
        "# Part-of-Speech Tagging\n",
        "pos_counts = {'NOUN': 0, 'VERB': 0, 'ADJ': 0, 'ADV': 0, 'PROPN': 0}\n",
        "for text in clean_texts:\n",
        "    try:\n",
        "        doc = nlp(str(text))\n",
        "        for token in doc:\n",
        "            try:\n",
        "                pos_counts[token.pos_] += 1\n",
        "            except KeyError:\n",
        "                pass\n",
        "    except ValueError:\n",
        "        pass\n",
        "\n",
        "# Save Part-of-Speech Counts to a file\n",
        "with open('pos_counts.txt', 'w') as f:\n",
        "    f.write(\"Part-of-Speech Counts:\\n\")\n",
        "    for pos, count in pos_counts.items():\n",
        "        f.write(f\"{pos}: {count}\\n\")\n",
        "\n",
        "# Constituency Parsing and Dependency Parsing\n",
        "constituency_trees = []\n",
        "dependency_trees = []\n",
        "for text in clean_texts:\n",
        "    try:\n",
        "        doc = nlp(str(text))\n",
        "        constituency_trees.append(list(doc.sents))\n",
        "        dependency_trees.append(doc.to_json())\n",
        "    except ValueError:\n",
        "        pass\n",
        "\n",
        "# Save Constituency Parsing Trees to a file\n",
        "with open('constituency_trees.txt', 'w') as f:\n",
        "    f.write(\"Constituency Parse Trees:\\n\")\n",
        "    for tree in constituency_trees:\n",
        "        f.write(f\"{tree}\\n\")\n",
        "\n",
        "# Save Dependency Parsing Trees to a file\n",
        "with open('dependency_trees.txt', 'w') as f:\n",
        "    f.write(\"Dependency Parse Trees:\\n\")\n",
        "    for tree in dependency_trees:\n",
        "        f.write(f\"{tree}\\n\")"
      ],
      "metadata": {
        "id": "32WKjZmcx5qU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8BFCvWp32cf"
      },
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_e557s2w4BpK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "ba9dd8a8-5696-4ee4-834a-ff80ee38d4af"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nThank you for providing such valuable questions that allowed me to enhance my understanding of web scraping, text preprocessing, and grammatical analysis. In the first question, we scraped data from the Densho website, collecting author names and their associated data. Our approach involved iterating through page URLs and handling errors for pages that didn't yield data. We then stored the collected data in a CSV file.\\nFor the second question, I utilized NLTK and other libraries to clean the text data. Handling null values in the dataset was crucial, and I implemented error handling to skip processing for these instances.\\nIn the third question, I employed a mixed approach using NLTK and spaCy for grammatical analysis. Error handling was again implemented to ensure smooth processing of the data.\\nOverall, these tasks were challenging but provided an excellent opportunity for learning and improving my skills. Thank you for the opportunity!\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Write your response below\n",
        "'''\n",
        "Thank you for providing such valuable questions that allowed me to enhance my understanding of web scraping, text preprocessing, and grammatical analysis. In the first question, we scraped data from the Densho website, collecting author names and their associated data. Our approach involved iterating through page URLs and handling errors for pages that didn't yield data. We then stored the collected data in a CSV file.\n",
        "For the second question, I utilized NLTK and other libraries to clean the text data. Handling null values in the dataset was crucial, and I implemented error handling to skip processing for these instances.\n",
        "In the third question, I employed a mixed approach using NLTK and spaCy for grammatical analysis. Error handling was again implemented to ensure smooth processing of the data.\n",
        "Overall, these tasks were challenging but provided an excellent opportunity for learning and improving my skills. Thank you for the opportunity!\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DfQMh36IzsG7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}